{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "cxN6TMvWB9jC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BroadCast(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def call(self, inputs, training = False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalUNET(tf.keras.models.Model):\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements a modified version of the UNET architecture. \n",
    "    UNET networks fall under the Fully-Convolutional-Neural-Networks (FCNN)\n",
    "    category, because they do not make use of Dense Layers. Instead, they rely \n",
    "    solely on convolution layers and they attempt to learn an image-to-image \n",
    "    mapping. \n",
    "\n",
    "    A typical UNET network consists of:\n",
    "    1) an encoder block (contracting path) with alternating Convolution and Downsampling layers (or strided Convolutions in some cases)\n",
    "    2) a decoder block (expanding path) with alternating Convolution and Upsampling layers (or Deconvolutions in some cases)\n",
    "    3) skip connections between all encoder-decoder levels with compatible dimensions\n",
    "\n",
    "    UNET models are widely used for semantic segmentation tasks, because they are usually able to \n",
    "    achieve good performance even in small datasets.\n",
    "\n",
    "    This particular implementation simply adds ConvLSTM layers in parallel to all skip connections between \n",
    "    the encoder and the decoder levels thus making it possible to extract temporal features from\n",
    "    sequences of satelite images at different resolution levels. The Convolution layers in the \n",
    "    encoder and decoder are left as is and they are applied independently for every timestep of the \n",
    "    input tensors. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_of_levels: int, \n",
    "        num_of_filters: list, \n",
    "        conv_blocks_per_level: int = 3, \n",
    "        kernel_size: tuple = (3,3), \n",
    "        leaky_relu_slope: float = 0.10):\n",
    "\n",
    "        \"\"\"\n",
    "        Class Constructor: \n",
    "\n",
    "        Arguments List: \n",
    "        -> num_of_levels: (int) Number of resolution levels in the encoder and decoder.\n",
    "        -> num_of_filters: (list of ints) Number of convolution kernels at every resolution level\n",
    "        -> conv_blocks_per_level: \n",
    "        -> kernel_size: (tuple of ints) Kernel dimensions in pixels [height x width]\n",
    "        -> leaky_relu_slope: ReLU slope for negative inputs.\n",
    "        \"\"\"\n",
    "\n",
    "        # Invoke the constructor of the base class.\n",
    "        super().__init__()\n",
    "        \n",
    "        self._num_of_levels = num_of_levels\n",
    "        \n",
    "        # Time-Distributed,  2D-Convolution layers on the encoder path \n",
    "        self._encoder_conv_blocks_2D = [None] * self._num_of_levels\n",
    "\n",
    "        # Time-Distributed, 2D-Convolution layers on the decoder path\n",
    "        self._decoder_conv_blocks_2D = [None] * self._num_of_levels\n",
    "\n",
    "        # 2D-Conv-LSTM layers between the encoder and decoder path \n",
    "        self._lstm_layers_2D = [None] * self._num_of_levels\n",
    "\n",
    "        # Skip connections (depth-wise concatenation) on the decoder path\n",
    "        self._concatenate_layers = [None] * self._num_of_levels\n",
    "        \n",
    "        # Downsampling layers on the encoder path \n",
    "        self._pooling_layers = [None] * self._num_of_levels\n",
    "\n",
    "        # Upsampling layers on the decoder path \n",
    "        self._upsampling_layers = [None] * self._num_of_levels\n",
    "\n",
    "        # Build the encoder network / contracting path \n",
    "        for i in range(self._num_of_levels): \n",
    "            self._encoder_conv_blocks_2D[i] = ConvBlock2D(\n",
    "                num_of_filters = num_of_filters[i],\n",
    "                num_of_blocks = conv_blocks_per_level, \n",
    "                kernel_size = kernel_size, \n",
    "                leaky_relu_slope = leaky_relu_slope                \n",
    "            )\n",
    "\n",
    "            # Wrap the previous Convolution Layer with the TimeDistributed layer to let Keras know \n",
    "            # that this convolution operation is meant to be applied independently for every frame\n",
    "            # of the input sequence.\n",
    "            self._encoder_conv_blocks_2D[i] = tf.keras.layers.TimeDistributed(self._encoder_conv_blocks_2D[i])\n",
    "\n",
    "            # The last resolution level on the encoder path does not require a Pooling layer \n",
    "            if i != self._num_of_levels - 1: \n",
    "                self._pooling_layers[i] = tf.keras.layers.MaxPooling2D(pool_size = (2,2), padding = \"valid\")\n",
    "                self._pooling_layers[i] = tf.keras.layers.TimeDistributed(self._pooling_layers[i])\n",
    "\n",
    "        # Add Conv2D-LSTM layers and residual connections between the corresponding encoder and decoder levels.\n",
    "        for i in range(self._num_of_levels):\n",
    "            self._lstm_layers_2D[i] = ConvBlockLSTM(\n",
    "                num_of_filters = num_of_filters[i], \n",
    "                kernel_size = kernel_size, \n",
    "                num_of_layers = 2, \n",
    "                bidirectional = False\n",
    "            )\n",
    "\n",
    "        # Build the decoder network / expanding path\n",
    "        for i in range(self._num_of_levels): \n",
    "            self._decoder_conv_blocks_2D[i] = ConvBlock2D(\n",
    "                num_of_filters = num_of_filters[i], \n",
    "                num_of_blocks = conv_blocks_per_level, \n",
    "                kernel_size = kernel_size, \n",
    "                leaky_relu_slope = leaky_relu_slope\n",
    "            )\n",
    "\n",
    "            # Wrap the previous Convolution Layer with the TimeDistributed layer, to let Keras know \n",
    "            # that this convolution operation is meant to be applied independently for every frame\n",
    "            # of the input sequence.\n",
    "            self._decoder_conv_blocks_2D[i] = tf.keras.layers.TimeDistributed(self._decoder_conv_blocks_2D[i])\n",
    "\n",
    "            # The first resolution level on the decoder path does not require an Upsampling layer\n",
    "            if i != 0: \n",
    "                self._upsampling_layers[i] = tf.keras.layers.UpSampling2D(size = (2,2), interpolation = \"bilinear\")\n",
    "                self._upsampling_layers[i] = tf.keras.layers.TimeDistributed(self._upsampling_layers[i])\n",
    "\n",
    "            # The last resolution level does not require a channel-wise, concatenation layer\n",
    "            if i != self._num_of_levels - 1: \n",
    "                self._concatenate_layers[i] = tf.keras.layers.Concatenate(axis = -1)\n",
    "\n",
    "        self._output_layer = tf.keras.layers.Conv3D(\n",
    "            filters = 1,\n",
    "            kernel_size = (3,3,3),\n",
    "            padding = \"same\",\n",
    "            activation = \"sigmoid\"\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, training = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Description:\n",
    "        The 'call' method describes the computation graph of a model.\n",
    "        First, the input-stream of satellite images is processed by the \n",
    "        convolution layers of the encoder. Then, the output of every \n",
    "        encoder-stage is fed to the corresponding ConvLSTM layer. The \n",
    "        output of these ConvLSTM layers is then fed to the decoder of the\n",
    "        architecture. Pooling and Upsampling connections are used to \n",
    "        connect the different resolution levels of the encoder and decoder\n",
    "        blocks. Finally, a 3D Convolution layer is used to produce the \n",
    "        segmentation masks of the next timesteps.\n",
    "\n",
    "        Arguments List: \n",
    "         -> inputs: (tensor) The input of the model. A 5D tensor with the following dimensions: {batch, timestep, height, width, channels}\n",
    "         -> training: (bool) True indicates that the model is in 'training' mode whereas False indicates that the model is in 'inference' \n",
    "            mode. For most layers this makes no difference, however certain types of layers (such as batch-norm layers) need this information \n",
    "            to work as expected.\n",
    "\n",
    "        Return List: \n",
    "         -> output: 5D tensor { batch x timesteps x height x width x 1 } Model predictions (future segmentation masks)\n",
    "        \"\"\"\n",
    "\n",
    "        temp = inputs\n",
    "        \n",
    "        encoder_outputs = [None] * self._num_of_levels\n",
    "        lstm_outputs = [None] * self._num_of_levels\n",
    "        decoder_outputs = [None] * self._num_of_levels\n",
    "\n",
    "        # Define the computation graph of the contracting path / encoder network\n",
    "        for i in range(self._num_of_levels):             \n",
    "            encoder_outputs[i] = self._encoder_conv_blocks_2D[i](temp, training = training)\n",
    "\n",
    "            if i != self._num_of_levels - 1:\n",
    "                temp = self._pooling_layers[i](encoder_outputs[i])\n",
    "        \n",
    "        # Define the computation graph of the ConvLSTM layers between the encoder and decoder\n",
    "        for i in range(self._num_of_levels): \n",
    "            lstm_outputs[i] = self._lstm_layers_2D[i](encoder_outputs[i], training = training)\n",
    "        \n",
    "        # Define the computation graph of the expanding path / decoder network\n",
    "        for i in range(self._num_of_levels - 1, -1, -1):\n",
    "            \n",
    "            temp = lstm_outputs[i]\n",
    "            \n",
    "            # Concatenation\n",
    "            if i != self._num_of_levels - 1:\n",
    "                temp = self._concatenate_layers[i]([lstm_outputs[i], decoder_outputs[i+1]])\n",
    "            \n",
    "            # Convolution \n",
    "            decoder_outputs[i] = self._decoder_conv_blocks_2D[i](temp, training = training)\n",
    "\n",
    "            # Upsampling \n",
    "            if i != 0:\n",
    "                decoder_outputs[i] = self._upsampling_layers[i](decoder_outputs[i])\n",
    "\n",
    "        output_tensor = self._output_layer(decoder_outputs[0], training = training)\n",
    "        \n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer 'conv_block_lstm_5' (type ConvBlockLSTM).\n\nin user code:\n\n    File \"/tmp/ipykernel_294114/2684041515.py\", line 90, in call  *\n        for i in range(self._num_of_levels):\n\n    AttributeError: 'ConvBlockLSTM' object has no attribute '_num_of_levels'\n\n\nCall arguments received by layer 'conv_block_lstm_5' (type ConvBlockLSTM):\n  • inputs=tf.Tensor(shape=(None, 10, 512, 512, 8), dtype=float32)\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m TemporalUNET(\n\u001b[1;32m      2\u001b[0m     num_of_levels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, \n\u001b[1;32m      3\u001b[0m     num_of_filters \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m32\u001b[39m]\n\u001b[1;32m      4\u001b[0m ) \n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:542\u001b[0m, in \u001b[0;36mModel.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can only call `build()` on a model if its \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`call()` method accepts an `inputs` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m     )\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mInvalidArgumentError, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot build your model by calling `build` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif your layers do not support float type inputs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`call` is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[47], line 164\u001b[0m, in \u001b[0;36mTemporalUNET.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Define the computation graph of the ConvLSTM layers between the encoder and decoder\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_of_levels): \n\u001b[0;32m--> 164\u001b[0m     lstm_outputs[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lstm_layers_2D\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Define the computation graph of the expanding path / decoder network\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_of_levels \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filewwwtacxi.py:54\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     52\u001b[0m     temp \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(output)\n\u001b[1;32m     53\u001b[0m i \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m ag__\u001b[38;5;241m.\u001b[39mfor_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mrange\u001b[39m), (\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_of_levels\u001b[49m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), \u001b[38;5;28;01mNone\u001b[39;00m, loop_body, get_state_1, set_state_1, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m), {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterate_names\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling layer 'conv_block_lstm_5' (type ConvBlockLSTM).\n\nin user code:\n\n    File \"/tmp/ipykernel_294114/2684041515.py\", line 90, in call  *\n        for i in range(self._num_of_levels):\n\n    AttributeError: 'ConvBlockLSTM' object has no attribute '_num_of_levels'\n\n\nCall arguments received by layer 'conv_block_lstm_5' (type ConvBlockLSTM):\n  • inputs=tf.Tensor(shape=(None, 10, 512, 512, 8), dtype=float32)\n  • training=False"
     ]
    }
   ],
   "source": [
    "model = TemporalUNET(\n",
    "    num_of_levels = 5, \n",
    "    num_of_filters = [8, 12, 16, 24, 32]\n",
    ") \n",
    "\n",
    "model.build(input_shape=(None, 10, 512, 512, 2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
