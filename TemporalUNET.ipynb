{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cxN6TMvWB9jC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 15:33:28.829771: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-27 15:33:28.929315: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-27 15:33:28.929368: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-27 15:33:28.945746: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-27 15:33:28.989021: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 15:33:29.543317: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iVEssxu1ZBQU"
   },
   "outputs": [],
   "source": [
    "class ConvBlock2D(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    This class implemenents the basic convolution block of the architecture.\n",
    "    It consists of an input 'Conv2D[1x1]-BatchNorm-ReLU' block followed by a number\n",
    "    of hidden, 'Conv2D[nxn]-BatchNorm-ReLU' blocks. Residual connections are added \n",
    "    between the input and output of every hidden block. \n",
    "\n",
    "    - The (1x1) convolution-layer (sometimes referred to as depthwise convolution) can be used \n",
    "      for dimensionality reduction. This idea was popularized by the authors of the InceptionNet \n",
    "      architecture\n",
    "    \n",
    "    - Residual connections are used to achieve better gradient flow during training \n",
    "      This idea was popularized by the authors of the ResNET50 architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_of_filters: int,\n",
    "        num_of_blocks: int = 3,\n",
    "        kernel_size: tuple = (3,3),\n",
    "        leaky_relu_slope: float = 0.10 ):\n",
    "\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Class Constructor.\n",
    "\n",
    "        Arguments:\n",
    "         - num_of_filters: (int) number of convolution kernels per convolution layer.\n",
    "         - num_of_blocks: (int) number of Conv2D-BatchNorm-ReLU blocks. The default value is 3: 1 input block and 2 hidden blocks\n",
    "         - kernel_size: (tuple of ints) kernel dimensions for hidden convolution blocks in pixels (height x width). The default is (3,3)\n",
    "         - leaky_relu_slope: (float) ReLU slope for negative inputs. The default is 0.15 (A negative sign is implicitely assumed)\n",
    "        \"\"\"\n",
    "\n",
    "        assert(num_of_filters > 0)\n",
    "        assert(num_of_blocks > 1)\n",
    "        assert(len(kernel_size) == 2)\n",
    "        assert(kernel_size[0] > 0 and kernel_size[1] > 0)\n",
    "        \n",
    "        self._layer_name = \"[Conv2D[1x1]*{input_depth}-BatchNorm-ReLU]->[Conv2D[{height}x{width}]*{hidden_depth}-BatchNorm-ReLU]*{blocks}\".format(\n",
    "            input_depth = num_of_filters,\n",
    "            height = kernel_size[0], \n",
    "            width = kernel_size[1], \n",
    "            hidden_depth = num_of_filters, \n",
    "            blocks = num_of_blocks - 1\n",
    "        )\n",
    "        \n",
    "        # super().__init__( name = self._layer_name )\n",
    "        super().__init__()\n",
    "\n",
    "        # Total number of Conv2D-BN-ReLU blocks\n",
    "        self._num_of_blocks = num_of_blocks\n",
    "\n",
    "        # Lists for storing Conv2D, BatchNorm, ReLU and Residual layer instances\n",
    "        self._conv_layers = []\n",
    "        self._batch_norm_layers = [ tf.keras.layers.BatchNormalization() for i in range(self._num_of_blocks) ]\n",
    "        self._relu_layers = [ tf.keras.layers.LeakyReLU( alpha = abs(leaky_relu_slope)) for i in range(self._num_of_blocks) ]\n",
    "        self._residual_layers = [ tf.keras.layers.Add() if i > 0 else None for i in range(self._num_of_blocks) ]\n",
    "\n",
    "        # Optional Dropout layer (it is meant to be used after the last Conv2D-BN-ReLU block)\n",
    "        self._dropout_rate = 0.05\n",
    "        self._dropout_layer = tf.keras.layers.Dropout(rate = self._dropout_rate)\n",
    "        \n",
    "        for i in range(self._num_of_blocks):\n",
    "            self._conv_layers.append(\n",
    "                tf.keras.layers.SeparableConv2D(\n",
    "                    filters = num_of_filters,\n",
    "                    kernel_size = kernel_size if i > 0 else (1,1),\n",
    "                    padding = \"same\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, training = False):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        The 'call' method defines the computation graph of this custom layer.\n",
    "        The 'inputs' tensor first goes through the 1x1 Conv-BatchNorm-ReLU block. \n",
    "        The output of this computation is then fed to the hidden Conv-BatchNorm-ReLU blocks.\n",
    "        With the exception of the first 1x1 conv block, residual connections are used to between\n",
    "        the input and output of every hidden conv block. \n",
    "\n",
    "        Arguments List: \n",
    "        -> inputs: (4D tensor) {batch, height, width, channels}\n",
    "        -> training: (bool) Set to True during training. Set to False during inference mode\n",
    "\n",
    "        Returns: \n",
    "        -> output: (4D tensor) {batch, height, width, channels} The output of the final hidden block.\n",
    "           * 'batch', 'height' and 'width' are equal to the corresponding dimensions of the input-tensor.\n",
    "           * 'channels' is the equal to 'num_of_filters' argument provided in the constructor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholder variable for the input tensor of the current Conv2D-BN-ReLU block\n",
    "        previous_tensor = inputs\n",
    "\n",
    "        # Placeholder variable for the output tensor of the current Conv2D-BN-ReLU block\n",
    "        current_tensor = None\n",
    "\n",
    "        # Define the computation graph of this custom-layer\n",
    "        for i in range(self._num_of_blocks):\n",
    "\n",
    "            # Conv2D -> BatchNorm -> ReLU\n",
    "            current_tensor = self._conv_layers[i](previous_tensor)\n",
    "            current_tensor = self._batch_norm_layers[i](current_tensor, training = training)\n",
    "            current_tensor = self._relu_layers[i](current_tensor)\n",
    "\n",
    "            # Apply the Residual connection (the first Conv2D-BN-ReLU block does not use one)\n",
    "            if i > 0:\n",
    "                current_tensor = self._residual_layers[i]([current_tensor, previous_tensor])\n",
    "\n",
    "            # Save the output to use it as an input for the next iteration\n",
    "            previous_tensor = current_tensor\n",
    "\n",
    "        # Uncomment the following line to enable the optional Dropout layer\n",
    "        # return self._dropout_layer(current_tensor, training=training)\n",
    "        \n",
    "        # Return the final output tensor\n",
    "        return current_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlockLSTM(tf.keras.layers.Layer):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    This class implements the basic Conv-LSTM layer of the architecture. It consists\n",
    "    of multiple, stacked ConvLSTM layers with additional Residual connections and \n",
    "    Batch-Normalization layers in-between. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_of_filters: int, \n",
    "        kernel_size: tuple = (3,3),\n",
    "        num_of_layers: int = 1, \n",
    "        bidirectional: bool = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Class constructor: \n",
    "\n",
    "        Arguments List: \n",
    "        num_of_filters: (int) Number of filters per ConvLSTM layer.\n",
    "        kernel_size: (int) pixel dimensions for convolution kernels {height x width}. The default is [3x3] pixels.\n",
    "        num_of_layers: (int) Total number of stacked ConvLSTM layers. The default is 1 (single ConvLSTM layer)\n",
    "        bidirectional: (bool) if True, use bidirectional ConvLSTM layers. The default is False (feedforward, unidirectional layers).\n",
    "        \"\"\"\n",
    "\n",
    "        # Invoke the constructor of the base class (tf.keras.layers.Layer)\n",
    "        super().__init__()\n",
    "\n",
    "        # Placeholder attributes for the arguments of the constructor\n",
    "        self._num_of_layers = num_of_layers\n",
    "        self._conv_lstm_layers = [None] * self._num_of_layers\n",
    "        self._batch_norm_layers = [None] * self._num_of_layers\n",
    "        self._residual_layers = [None] * self._num_of_layers\n",
    "\n",
    "        # Stack multiple ConvLSTM Layers. \n",
    "        for i in range(self._num_of_layers):\n",
    "\n",
    "            # (Time-Distributed) Batch-Normalization layers\n",
    "            self._batch_norm_layers[i] = tf.keras.layers.BatchNormalization()\n",
    "            self._batch_norm_layers[i] = tf.keras.layers.TimeDistributed(self._batch_norm_layers[i])\n",
    "\n",
    "            # ConvLSTM layers\n",
    "            self._conv_lstm_layers[i] = tf.keras.layers.ConvLSTM2D(\n",
    "                filters = num_of_filters,\n",
    "                kernel_size = kernel_size, \n",
    "                padding = \"same\",\n",
    "                return_sequences = True, \n",
    "                return_state = False, \n",
    "                dropout = 0.0,\n",
    "                recurrent_dropout = 0.0,\n",
    "            )\n",
    "\n",
    "            # If the bidirectional option is enabled, wrap the ConvLSTM layer with the BiDirectional layer\n",
    "            if bidirectional == True: \n",
    "                self._conv_lstm_layers[i] = tf.keras.layers.BiDirectional(\n",
    "                    layer = self._conv_lstm_layers[i],\n",
    "                    merge_mode = 'sum',\n",
    "                    backward_layer = None           # TODO: Use a distinct ConvLSTM layer for the backward direction (maybe?)\n",
    "                )\n",
    "\n",
    "            # Residual connection (channel-wise addition) between the input and \n",
    "            # the output of the ConvLSTM layer.\n",
    "            # The first ConvLSTM layer cannot have a residual connection, since we don't \n",
    "            # know in advance how many feature-maps its input tensor has.\n",
    "            if i > 0: \n",
    "                self._residual_layers[i] = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, inputs, training = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Description: \n",
    "        The 'call' method defines the computation graph of this custom layer. \n",
    "\n",
    "        Arguments List: \n",
    "        -> inputs:  (5D tensor) { batch, timestep, height, width, channels }\n",
    "        -> training: (bool) \n",
    "\n",
    "        Returns: \n",
    "        -> output: (5D tensor) { batch, timestep, height, width, channels }\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholder variable for the output of the previous iteration\n",
    "        temp = inputs\n",
    "\n",
    "        # Placeholder variable for the output of the current iteration\n",
    "        output = None\n",
    "\n",
    "        # Define the computation graph of the stacked ConvLSTM layer\n",
    "        for i in range(self._num_of_levels):\n",
    "            \n",
    "            # Batch-Norm -> ConvLSTM \n",
    "            output = self._batch_norm_layers[i](temp, training = training)\n",
    "            output = self._conv_lstm_layers[i](output, training = training)\n",
    "\n",
    "            # All ConvLSTM layers use a Residual connection with the exception of the first one.\n",
    "            if i > 0: \n",
    "                output = self._residual_layers[i]([output, temp])\n",
    "\n",
    "            # Save the output tensor of this iteration and used it as input for the next one\n",
    "            temp = output\n",
    "\n",
    "        # Return the output tensor of the last iteration\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BroadCast(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def call(self, inputs, training = False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalUNET(tf.keras.models.Model):\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements a modified version of the UNET architecture. \n",
    "    UNET networks fall under the Fully-Convolutional-Neural-Networks (FCNN)\n",
    "    category, because they do not make use of Dense Layers. Instead, they rely \n",
    "    solely on convolution layers and they attempt to learn an image-to-image \n",
    "    mapping. \n",
    "\n",
    "    A typical UNET network consists of:\n",
    "    1) an encoder block (contracting path) with alternating Convolution and Downsampling layers (or strided Convolutions in some cases)\n",
    "    2) a decoder block (expanding path) with alternating Convolution and Upsampling layers (or Deconvolutions in some cases)\n",
    "    3) skip connections between all encoder-decoder levels with compatible dimensions\n",
    "\n",
    "    UNET models are widely used for semantic segmentation tasks, because they are usually able to \n",
    "    achieve good performance even in small datasets.\n",
    "\n",
    "    This particular implementation simply adds ConvLSTM layers in parallel to all skip connections between \n",
    "    the encoder and the decoder levels thus making it possible to extract temporal features from\n",
    "    sequences of satelite images at different resolution levels. The Convolution layers in the \n",
    "    encoder and decoder are left as is and they are applied independently for every timestep of the \n",
    "    input tensors. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_of_levels: int, \n",
    "        num_of_filters: list, \n",
    "        conv_blocks_per_level: int = 3, \n",
    "        kernel_size: tuple = (3,3), \n",
    "        leaky_relu_slope: float = 0.10):\n",
    "\n",
    "        \"\"\"\n",
    "        Class Constructor: \n",
    "\n",
    "        Arguments List: \n",
    "        -> num_of_levels: (int) Number of resolution levels in the encoder and decoder.\n",
    "        -> num_of_filters: (list of ints) Number of convolution kernels at every resolution level\n",
    "        -> conv_blocks_per_level: \n",
    "        -> kernel_size: (tuple of ints) Kernel dimensions in pixels [height x width]\n",
    "        -> leaky_relu_slope: ReLU slope for negative inputs.\n",
    "        \"\"\"\n",
    "\n",
    "        # Invoke the constructor of the base class.\n",
    "        super().__init__()\n",
    "        \n",
    "        self._num_of_levels = num_of_levels\n",
    "        \n",
    "        # Time-Distributed,  2D-Convolution layers on the encoder path \n",
    "        self._encoder_conv_blocks_2D = [None] * self._num_of_levels\n",
    "\n",
    "        # Time-Distributed, 2D-Convolution layers on the decoder path\n",
    "        self._decoder_conv_blocks_2D = [None] * self._num_of_levels\n",
    "\n",
    "        # 2D-Conv-LSTM layers between the encoder and decoder path \n",
    "        self._lstm_layers_2D = [None] * self._num_of_levels\n",
    "\n",
    "        # Skip connections (depth-wise concatenation) on the decoder path\n",
    "        self._concatenate_layers = [None] * self._num_of_levels\n",
    "        \n",
    "        # Downsampling layers on the encoder path \n",
    "        self._pooling_layers = [None] * self._num_of_levels\n",
    "\n",
    "        # Upsampling layers on the decoder path \n",
    "        self._upsampling_layers = [None] * self._num_of_levels\n",
    "\n",
    "        # Build the encoder network / contracting path \n",
    "        for i in range(self._num_of_levels): \n",
    "            self._encoder_conv_blocks_2D[i] = ConvBlock2D(\n",
    "                num_of_filters = num_of_filters[i],\n",
    "                num_of_blocks = conv_blocks_per_level, \n",
    "                kernel_size = kernel_size, \n",
    "                leaky_relu_slope = leaky_relu_slope                \n",
    "            )\n",
    "\n",
    "            # Wrap the previous Convolution Layer with the TimeDistributed layer to let Keras know \n",
    "            # that this convolution operation is meant to be applied independently for every frame\n",
    "            # of the input sequence.\n",
    "            self._encoder_conv_blocks_2D[i] = tf.keras.layers.TimeDistributed(self._encoder_conv_blocks_2D[i])\n",
    "\n",
    "            # The last resolution level on the encoder path does not require a Pooling layer \n",
    "            if i != self._num_of_levels - 1: \n",
    "                self._pooling_layers[i] = tf.keras.layers.MaxPooling2D(pool_size = (2,2), padding = \"valid\")\n",
    "                self._pooling_layers[i] = tf.keras.layers.TimeDistributed(self._pooling_layers[i])\n",
    "\n",
    "        # Add Conv2D-LSTM layers and residual connections between the corresponding encoder and decoder levels.\n",
    "        for i in range(self._num_of_levels):\n",
    "            self._lstm_layers_2D[i] = ConvBlockLSTM(\n",
    "                num_of_filters = num_of_filters[i], \n",
    "                kernel_size = kernel_size, \n",
    "                num_of_layers = 2, \n",
    "                bidirectional = False\n",
    "            )\n",
    "\n",
    "        # Build the decoder network / expanding path\n",
    "        for i in range(self._num_of_levels): \n",
    "            self._decoder_conv_blocks_2D[i] = ConvBlock2D(\n",
    "                num_of_filters = num_of_filters[i], \n",
    "                num_of_blocks = conv_blocks_per_level, \n",
    "                kernel_size = kernel_size, \n",
    "                leaky_relu_slope = leaky_relu_slope\n",
    "            )\n",
    "\n",
    "            # Wrap the previous Convolution Layer with the TimeDistributed layer, to let Keras know \n",
    "            # that this convolution operation is meant to be applied independently for every frame\n",
    "            # of the input sequence.\n",
    "            self._decoder_conv_blocks_2D[i] = tf.keras.layers.TimeDistributed(self._decoder_conv_blocks_2D[i])\n",
    "\n",
    "            # The first resolution level on the decoder path does not require an Upsampling layer\n",
    "            if i != 0: \n",
    "                self._upsampling_layers[i] = tf.keras.layers.UpSampling2D(size = (2,2), interpolation = \"bilinear\")\n",
    "                self._upsampling_layers[i] = tf.keras.layers.TimeDistributed(self._upsampling_layers[i])\n",
    "\n",
    "            # The last resolution level does not require a channel-wise, concatenation layer\n",
    "            if i != self._num_of_levels - 1: \n",
    "                self._concatenate_layers[i] = tf.keras.layers.Concatenate(axis = -1)\n",
    "\n",
    "        self._output_layer = tf.keras.layers.Conv3D(\n",
    "            filters = 1,\n",
    "            kernel_size = (3,3,3),\n",
    "            padding = \"same\",\n",
    "            activation = \"sigmoid\"\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, training = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Description:\n",
    "        The 'call' method describes the computation graph of a model.\n",
    "        First, the input-stream of satellite images is processed by the \n",
    "        convolution layers of the encoder. Then, the output of every \n",
    "        encoder-stage is fed to the corresponding ConvLSTM layer. The \n",
    "        output of these ConvLSTM layers is then fed to the decoder of the\n",
    "        architecture. Pooling and Upsampling connections are used to \n",
    "        connect the different resolution levels of the encoder and decoder\n",
    "        blocks. Finally, a 3D Convolution layer is used to produce the \n",
    "        segmentation masks of the next timesteps.\n",
    "\n",
    "        Arguments List: \n",
    "         -> inputs: (tensor) The input of the model. A 5D tensor with the following dimensions: {batch, timestep, height, width, channels}\n",
    "         -> training: (bool) True indicates that the model is in 'training' mode whereas False indicates that the model is in 'inference' \n",
    "            mode. For most layers this makes no difference, however certain types of layers (such as batch-norm layers) need this information \n",
    "            to work as expected.\n",
    "\n",
    "        Return List: \n",
    "         -> output: 5D tensor { batch x timesteps x height x width x 1 } Model predictions (future segmentation masks)\n",
    "        \"\"\"\n",
    "\n",
    "        temp = inputs\n",
    "        \n",
    "        encoder_outputs = [None] * self._num_of_levels\n",
    "        lstm_outputs = [None] * self._num_of_levels\n",
    "        decoder_outputs = [None] * self._num_of_levels\n",
    "\n",
    "        # Define the computation graph of the contracting path / encoder network\n",
    "        for i in range(self._num_of_levels):             \n",
    "            encoder_outputs[i] = self._encoder_conv_blocks_2D[i](temp, training = training)\n",
    "\n",
    "            if i != self._num_of_levels - 1:\n",
    "                temp = self._pooling_layers[i](encoder_outputs[i])\n",
    "        \n",
    "        # Define the computation graph of the ConvLSTM layers between the encoder and decoder\n",
    "        for i in range(self._num_of_levels): \n",
    "            lstm_outputs[i] = self._lstm_layers_2D[i](encoder_outputs[i], training = training)\n",
    "        \n",
    "        # Define the computation graph of the expanding path / decoder network\n",
    "        for i in range(self._num_of_levels - 1, -1, -1):\n",
    "            \n",
    "            temp = lstm_outputs[i]\n",
    "            \n",
    "            # Concatenation\n",
    "            if i != self._num_of_levels - 1:\n",
    "                temp = self._concatenate_layers[i]([lstm_outputs[i], decoder_outputs[i+1]])\n",
    "            \n",
    "            # Convolution \n",
    "            decoder_outputs[i] = self._decoder_conv_blocks_2D[i](temp, training = training)\n",
    "\n",
    "            # Upsampling \n",
    "            if i != 0:\n",
    "                decoder_outputs[i] = self._upsampling_layers[i](decoder_outputs[i])\n",
    "\n",
    "        output_tensor = self._output_layer(decoder_outputs[0], training = training)\n",
    "        \n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder path: 0\n",
      "(None, 10, 512, 512, 8)\n",
      "encoder path: 1\n",
      "(None, 10, 256, 256, 12)\n",
      "encoder path: 2\n",
      "(None, 10, 128, 128, 16)\n",
      "encoder path: 3\n",
      "(None, 10, 64, 64, 24)\n",
      "encoder path: 4\n",
      "(None, 10, 32, 32, 32)\n",
      "\n",
      "LSTM path: 0\n",
      "(None, 10, 512, 512, 8)\n",
      "LSTM path: 1\n",
      "(None, 10, 256, 256, 12)\n",
      "LSTM path: 2\n",
      "(None, 10, 128, 128, 16)\n",
      "LSTM path: 3\n",
      "(None, 10, 64, 64, 24)\n",
      "LSTM path: 4\n",
      "(None, 10, 32, 32, 32)\n",
      "\n",
      "Decoder path: 4\n",
      "concat shape: (None, 10, 32, 32, 32)\n",
      "conv shape: (None, 10, 32, 32, 32)\n",
      "upsampling size: (None, 10, 64, 64, 32)\n",
      "Decoder path: 3\n",
      "concat shape: (None, 10, 64, 64, 56)\n",
      "conv shape: (None, 10, 64, 64, 24)\n",
      "upsampling size: (None, 10, 128, 128, 24)\n",
      "Decoder path: 2\n",
      "concat shape: (None, 10, 128, 128, 40)\n",
      "conv shape: (None, 10, 128, 128, 16)\n",
      "upsampling size: (None, 10, 256, 256, 16)\n",
      "Decoder path: 1\n",
      "concat shape: (None, 10, 256, 256, 28)\n",
      "conv shape: (None, 10, 256, 256, 12)\n",
      "upsampling size: (None, 10, 512, 512, 12)\n",
      "Decoder path: 0\n",
      "concat shape: (None, 10, 512, 512, 20)\n",
      "conv shape: (None, 10, 512, 512, 8)\n",
      "upsampling size: (None, 10, 512, 512, 8)\n",
      "\n",
      "tensor output: (None, 512, 512, 2)\n",
      "Model: \"temporal_unet_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_90 (TimeD  multiple                  410       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_92 (TimeD  multiple                  788       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_94 (TimeD  multiple                  1244      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_96 (TimeD  multiple                  2344      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_98 (TimeD  multiple                  3896      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_99 (TimeD  multiple                  572       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_100 (Time  multiple                  1048      \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " time_distributed_102 (Time  multiple                  1720      \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " time_distributed_104 (Time  multiple                  3344      \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " time_distributed_106 (Time  multiple                  4160      \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " conv_lstm2d_30 (ConvLSTM2D  multiple                  4640      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_31 (ConvLSTM2D  multiple                  10416     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_32 (ConvLSTM2D  multiple                  18496     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_33 (ConvLSTM2D  multiple                  41568     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_34 (ConvLSTM2D  multiple                  73856     \n",
      " )                                                               \n",
      "                                                                 \n",
      " add_135 (Add)               multiple                  0         \n",
      "                                                                 \n",
      " add_136 (Add)               multiple                  0         \n",
      "                                                                 \n",
      " add_137 (Add)               multiple                  0         \n",
      "                                                                 \n",
      " add_138 (Add)               multiple                  0         \n",
      "                                                                 \n",
      " add_139 (Add)               multiple                  0         \n",
      "                                                                 \n",
      " concatenate_20 (Concatenat  multiple                  0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " concatenate_21 (Concatenat  multiple                  0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " concatenate_22 (Concatenat  multiple                  0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " concatenate_23 (Concatenat  multiple                  0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " time_distributed_91 (TimeD  multiple                  0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_93 (TimeD  multiple                  0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_95 (TimeD  multiple                  0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_97 (TimeD  multiple                  0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_101 (Time  multiple                  0         \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " time_distributed_103 (Time  multiple                  0         \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " time_distributed_105 (Time  multiple                  0         \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " time_distributed_107 (Time  multiple                  0         \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " conv_lstm2d_35 (ConvLSTM2D  multiple                  728       \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 169230 (661.05 KB)\n",
      "Trainable params: 168126 (656.74 KB)\n",
      "Non-trainable params: 1104 (4.31 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TemporalUNET(\n",
    "    num_of_levels = 5, \n",
    "    num_of_filters = [8, 12, 16, 24, 32]\n",
    ") \n",
    "\n",
    "model.build(input_shape=(None, 10, 512, 512, 2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinaryJaccardIndex(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    epsilon = 1e-5, \n",
    "    axis = -1, \n",
    "    use_cloud_mask = True, \n",
    "    cloud_threshold = 0.5,\n",
    "    num_of_prediction_steps = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    The Jaccard Index (also known as Intersection-over-Union, IoU) is a \n",
    "    commonly used metric in image segmentation tasks. \n",
    "\n",
    "    Given two segmentation masks A and B, the IoU is defined as: \n",
    "      IoU(A,B) = intersection(A,B) / union(A,B)\n",
    "\n",
    "    IoU values close to 1, indicate good overlap between A and B\n",
    "    IoU values close to 0, indicate poor overlap between A and B\n",
    "\n",
    "    Arguments List:\n",
    "    -> y_true: (5D tensor) Ground-truth segmentation mask + optional cloud-probability masks { batch x timesteps x height x width x 2 }\n",
    "    -> y_pred: (5D tensor) Predicted segmentation mask { batch x timesteps x height x width x 1 } \n",
    "    -> epsilon: (float) small positive constant for numeric stability in tensor division operations\n",
    "    -> axis: (int)\n",
    "    -> use_cloud_mask: (bool) if True, all pixels with a cloud-probability greater than 'cloud_threshold' are not taken into account\n",
    "       The default is True\n",
    "    -> cloud_threshold: (float) See above\n",
    "    -> num_of_prediction_steps: (int) Number of future prediction steps to take into account when calculating this metric. The default\n",
    "       is 1 which means that only the next prediction step is taken into account.\n",
    "\n",
    "    Returns:\n",
    "    -> IoU: scalar, 1D tensor\n",
    "\n",
    "    References: \n",
    "    -> https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/losses/jaccard.py\n",
    "    \"\"\"\n",
    "\n",
    "    # Flatten tensors down to 1 dimension\n",
    "    y_true = tf.keras.backend.flatten(y_true)\n",
    "    y_pred = tf.keras.backend.flatten(y_pred)\n",
    "\n",
    "    # Intersection (Overlap between ground-truth and predictions)\n",
    "    intersection = tf.keras.backend.abs(y_true * y_pred)\n",
    "    intersection = tf.keras.backend.sum(intersection, axis=axis)\n",
    "\n",
    "    # Union (Required for scaling / normalization step)\n",
    "    union = tf.keras.backend.abs(y_true) + tf.keras.backend.abs(y_pred)\n",
    "    union = tf.keras.backend.sum(union, axis=axis)\n",
    "    union = union - intersection\n",
    "\n",
    "    return  intersection / (union + epsilon)\n",
    "\n",
    "def BinaryJaccardLoss(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    smoothing = 1e-5, \n",
    "    axis = -1, \n",
    "    use_cloud_mask = False, \n",
    "    num_of_prediction_steps = 1): \n",
    "    \n",
    "    \"\"\"\n",
    "    Description: \n",
    "    The Jaccard Loss function is defined as: \n",
    "      Jac(A,B) = 1 - IoU(A,B)\n",
    "\n",
    "    Since all keras optimizers work by minimizing a loss function, \n",
    "    the Jaccard Loss function should be used when attempting to \n",
    "    maximize the IoU metric of a prediction model.\n",
    "\n",
    "    Arguments List: \n",
    "    -> y_true: (tensor)\n",
    "    -> y_pred: (tensor) \n",
    "    -> smoothing: (float)\n",
    "    -> axis: (int)\n",
    "    -> use_cloud_mask: (bool) \n",
    "    -> num_of_prediction_steps: int\n",
    "\n",
    "    Returns: \n",
    "    -> loss: scalar 1D tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # Flatten tensors down to 1 dimension\n",
    "    y_true = tf.keras.backend.flatten(y_true)\n",
    "    y_pred = tf.keras.backend.flatten(y_pred)\n",
    "\n",
    "    # \"Fuzzy\" Intersection (Overlap between ground-truth and predictions)\n",
    "    intersection = tf.keras.backend.abs(y_true * y_pred)\n",
    "    intersection = tf.keras.backend.sum(intersection, axis=axis)\n",
    "\n",
    "    # \"Fuzzy\" Union (Required for scaling / normalization step)\n",
    "    union = tf.keras.backend.abs(y_true) + tf.keras.backend.abs(y_pred)\n",
    "    union = tf.keras.backend.sum(union, axis=axis)\n",
    "    union = union - intersection\n",
    "\n",
    "    # Intersection-over-Union (Normalized overlap between ground-truth and predictions)\n",
    "    IoU = (intersection + smoothing) / (union + smoothing) \n",
    "\n",
    "    return 1 - IoU \n",
    "\n",
    "def BinaryDiceCoefficient(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    epsilon = 1e-5, \n",
    "    axis = -1, \n",
    "    use_cloud_mask = False, \n",
    "    num_of_prediction_steps = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Much like the Jaccard Index, the Dice coefficient is also used \n",
    "    in image segmentation tasks to measure the similarity between two\n",
    "    segmentation maps. \n",
    "\n",
    "    Given two segmentation masks A and B, the Dice coefficient is defined as: \n",
    "      Dice(A,B) = 2 * intersection(A,B) / ( union(A,B) + intersection(A,B) )\n",
    "\n",
    "    Values close to 1 indicate strong agreement between A and B, whereas \n",
    "    values close to 0 indicate poor agreement between A and B.\n",
    "\n",
    "    Arguments List:\n",
    "    -> y_true: (tensor)\n",
    "    -> y_pred: (tensor)\n",
    "    -> epsilon: (float)\n",
    "    -> axis: (int)\n",
    "    -> use_cloud_mask: (bool)\n",
    "\n",
    "    Returns:\n",
    "    -> coeff: scalar 1D tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # Flatten tensors down to 1 dimension\n",
    "    y_true = tf.keras.backend.flatten(y_true)\n",
    "    y_pred = tf.keras.backend.flatten(y_pred)\n",
    "\n",
    "    # \"Fuzzy\" Intersection (Overlap between ground-truth and predictions)\n",
    "    intersection = tf.keras.backend.abs(y_true * y_pred)\n",
    "    intersection = tf.keras.backend.sum(intersection, axis = axis)\n",
    "\n",
    "    # \"Fuzzy\" Union (Required for scaling / normalization step)\n",
    "    union = tf.keras.backend.abs(y_true) + tf.keras.backend.abs(y_pred)\n",
    "    union = tf.keras.backend.sum(union, axis = axis)\n",
    "    union = union - intersection\n",
    "\n",
    "    # Dice Coefficient \n",
    "    return ( 2 * intersection ) / ( union + intersection + epsilon )\n",
    "\n",
    "def BinaryDiceLoss(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    smoothing = 1e-5, \n",
    "    use_cloud_mask = False, \n",
    "    num_of_prediction_steps = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description: \n",
    "    The Dice Loss is defined as: \n",
    "      DiceLoss(A, B) = 1 - DiceCoeff(A,B)\n",
    "\n",
    "    Since keras optimizers expect a loss function to minimize, \n",
    "    the Dice Loss function should be used when attempting to maximize\n",
    "    the Dice coefficient of a prediction model.\n",
    "\n",
    "    Arguments List:\n",
    "    -> y_true: (tensor) Ground-truth segmentation masks as a binary tensor.\n",
    "    -> y_pred: (tensor) Model predictions.\n",
    "    -> smoothing: (float) \n",
    "    -> use_cloud_mask: (bool)\n",
    "\n",
    "    Returns:\n",
    "    -> loss: scalar 1D tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # Flatten tensors down to 1D vectors\n",
    "    y_true = tf.keras.backend.flatten(y_true)\n",
    "    y_pred = tf.keras.backend.flatten(y_pred)\n",
    "\n",
    "    # \"Fuzzy\" Intersection (Overlap between ground-truth and predictions)\n",
    "    intersection = tf.keras.backend.abs(y_true * y_pred)\n",
    "    intersection = tf.keras.backend.sum(intersection, axis = axis)\n",
    "\n",
    "    # \"Fuzzy\" Union (Required for normalization)\n",
    "    union = tf.keras.backend.abs(y_true) + tf.keras.backend.abs(y_pred)\n",
    "    union = tf.keras.backend.sum(union, axis = axis)\n",
    "    union = union - intersection\n",
    "\n",
    "    # Dice coefficient with smoothing factor\n",
    "    dice = ( 2 * intersection + smoothing ) / ( union + intersection + smoothing ) \n",
    "\n",
    "    # Dice Loss\n",
    "    return 1 - dice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "imgs = np.random.randint(0, 255, (100, 50, 50, 3), dtype=np.uint8)\n",
    "\n",
    "imgs[:,:,:,0] = imgs[:,:,:,0]\n",
    "imgs[:,:,:,1] = imgs[:,:,:,0]\n",
    "imgs[:,:,:,2] = imgs[:,:,:,0]\n",
    "\n",
    "imgs = [Image.fromarray(img) for img in imgs]\n",
    "\n",
    "# duration is the number of milliseconds between frames; this is 40 frames per second\n",
    "imgs[0].save(\"array.gif\", save_all=True, append_images=imgs[1:], duration=50, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function save in module PIL.Image:\n",
      "\n",
      "save(self, fp, format=None, **params) -> 'None'\n",
      "    Saves this image under the given filename.  If no format is\n",
      "    specified, the format to use is determined from the filename\n",
      "    extension, if possible.\n",
      "    \n",
      "    Keyword options can be used to provide additional instructions\n",
      "    to the writer. If a writer doesn't recognise an option, it is\n",
      "    silently ignored. The available options are described in the\n",
      "    :doc:`image format documentation\n",
      "    <../handbook/image-file-formats>` for each writer.\n",
      "    \n",
      "    You can use a file object instead of a filename. In this case,\n",
      "    you must always specify the format. The file object must\n",
      "    implement the ``seek``, ``tell``, and ``write``\n",
      "    methods, and be opened in binary mode.\n",
      "    \n",
      "    :param fp: A filename (string), pathlib.Path object or file object.\n",
      "    :param format: Optional format override.  If omitted, the\n",
      "       format to use is determined from the filename extension.\n",
      "       If a file object was used instead of a filename, this\n",
      "       parameter should always be used.\n",
      "    :param params: Extra parameters to the image writer.\n",
      "    :returns: None\n",
      "    :exception ValueError: If the output format could not be determined\n",
      "       from the file name.  Use the format option to solve this.\n",
      "    :exception OSError: If the file could not be written.  The file\n",
      "       may have been created, and may contain partial data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def GIF(\n",
    "    arr, \n",
    "    cloud_mask = None, \n",
    "    label = 'mask'):\n",
    "    \n",
    "    temp = None\n",
    "    \n",
    "    if label == 'NDWI' or label == 'mNDWI' or label == 'NDVI':\n",
    "        temp = (arr + 1.0) * 255\n",
    "    elif label = 'VV' or label == 'VH':\n",
    "        temp = ( (arr + 50) / (50 + 10) ) * 255\n",
    "    elif label == 'mask':\n",
    "        temp = np.copy(arr)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid data format\")\n",
    "\n",
    "    temp[temp < 0] = 0\n",
    "    temp[temp > 255] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function save in module PIL.Image:\n",
      "\n",
      "save(self, fp, format=None, **params) -> 'None'\n",
      "    Saves this image under the given filename.  If no format is\n",
      "    specified, the format to use is determined from the filename\n",
      "    extension, if possible.\n",
      "    \n",
      "    Keyword options can be used to provide additional instructions\n",
      "    to the writer. If a writer doesn't recognise an option, it is\n",
      "    silently ignored. The available options are described in the\n",
      "    :doc:`image format documentation\n",
      "    <../handbook/image-file-formats>` for each writer.\n",
      "    \n",
      "    You can use a file object instead of a filename. In this case,\n",
      "    you must always specify the format. The file object must\n",
      "    implement the ``seek``, ``tell``, and ``write``\n",
      "    methods, and be opened in binary mode.\n",
      "    \n",
      "    :param fp: A filename (string), pathlib.Path object or file object.\n",
      "    :param format: Optional format override.  If omitted, the\n",
      "       format to use is determined from the filename extension.\n",
      "       If a file object was used instead of a filename, this\n",
      "       parameter should always be used.\n",
      "    :param params: Extra parameters to the image writer.\n",
      "    :returns: None\n",
      "    :exception ValueError: If the output format could not be determined\n",
      "       from the file name.  Use the format option to solve this.\n",
      "    :exception OSError: If the file could not be written.  The file\n",
      "       may have been created, and may contain partial data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(PIL.Image.Image.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
