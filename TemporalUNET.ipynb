{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cxN6TMvWB9jC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 17:46:38.386883: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-09 17:46:38.406939: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-09 17:46:38.406959: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-09 17:46:38.407503: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-09 17:46:38.411365: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-09 17:46:38.788917: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iVEssxu1ZBQU"
   },
   "outputs": [],
   "source": [
    "class ConvBlock2D(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    This class implemenents the basic convolution block of the architecture.\n",
    "    It consists of an input 'Conv2D[1x1]-BatchNorm-ReLU' followed by a number\n",
    "    of hidden 'Conv2D[nxn]-BatchNorm-ReLU' blocks. Residual connections are added \n",
    "    between the input and output of every hidden block. \n",
    "\n",
    "    - The (1x1) convolution-layer can be used for dimensionality reduction. \n",
    "      This idea was popularized by the authors of the InceptionNet architecture\n",
    "    \n",
    "    - Residual connections are used to promote better gradient flow during training \n",
    "      This idea was popularized by the authors of the ResNET50 architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_of_filters: int,\n",
    "        num_of_blocks: int = 3,\n",
    "        kernel_size: tuple = (3,3),\n",
    "        leaky_relu_slope: float = 0.10 ):\n",
    "\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Class Constructor.\n",
    "\n",
    "        Arguments:\n",
    "         - num_of_filters: (int) number of convolution kernels per convolution layer.\n",
    "         - num_of_blocks: (int) number of Conv2D-BatchNorm-ReLU blocks. The default value is 3: 1 input block and 2 hidden blocks\n",
    "         - kernel_size: (tuple of ints) kernel dimensions for hidden convolution blocks in pixels (height x width). The default is (3,3)\n",
    "         - leaky_relu_slope: (float) ReLU slope for negative inputs. The default is 0.15 (A negative sign is implicitely assumed)\n",
    "        \"\"\"\n",
    "\n",
    "        assert(num_of_filters > 0)\n",
    "        assert(num_of_blocks > 1)\n",
    "        assert(len(kernel_size) == 2)\n",
    "        assert(kernel_size[0] > 0 and kernel_size[1] > 0)\n",
    "        \n",
    "        self._layer_name = \"[Conv2D[1x1]*{input_depth}-BatchNorm-ReLU]->[Conv2D[{height}x{width}]*{hidden_depth}-BatchNorm-ReLU]*{blocks}\".format(\n",
    "            input_depth = num_of_filters,\n",
    "            height = kernel_size[0], \n",
    "            width = kernel_size[1], \n",
    "            hidden_depth = num_of_filters, \n",
    "            blocks = num_of_blocks - 1\n",
    "        )\n",
    "        \n",
    "        # super().__init__( name = self._layer_name )\n",
    "        super().__init__()\n",
    "\n",
    "        # Total number of Conv2D-BN-ReLU blocks\n",
    "        self._num_of_blocks = num_of_blocks\n",
    "\n",
    "        # Lists for storing Conv2D, BatchNorm, ReLU and Residual layer instances\n",
    "        self._conv_layers = []\n",
    "        self._batch_norm_layers = [ tf.keras.layers.BatchNormalization() for i in range(self._num_of_blocks) ]\n",
    "        self._relu_layers = [ tf.keras.layers.LeakyReLU( alpha = abs(leaky_relu_slope)) for i in range(self._num_of_blocks) ]\n",
    "        self._residual_layers = [ tf.keras.layers.Add() if i > 0 else None for i in range(self._num_of_blocks) ]\n",
    "\n",
    "        # Optional Dropout layer (it is meant to be used after the last Conv2D-BN-ReLU block)\n",
    "        self._dropout_rate = 0.05\n",
    "        self._dropout_layer = tf.keras.layers.Dropout(rate = self._dropout_rate)\n",
    "        \n",
    "        for i in range(self._num_of_blocks):\n",
    "            self._conv_layers.append(\n",
    "                tf.keras.layers.SeparableConv2D(\n",
    "                    filters = num_of_filters,\n",
    "                    kernel_size = kernel_size if i > 0 else (1,1),\n",
    "                    padding = \"same\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, training = False):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholder variable for the input tensor of the current Conv2D-BN-ReLU block\n",
    "        previous_tensor = inputs\n",
    "\n",
    "        # Placeholder variable for the output tensor of the current Conv2D-BN-ReLU block\n",
    "        current_tensor = None\n",
    "\n",
    "        # Define the computation graph of this custom-layer\n",
    "        for i in range(self._num_of_blocks):\n",
    "\n",
    "            # Conv2D -> BatchNorm -> ReLU\n",
    "            current_tensor = self._conv_layers[i](previous_tensor)\n",
    "            current_tensor = self._batch_norm_layers[i](current_tensor, training = training)\n",
    "            current_tensor = self._relu_layers[i](current_tensor)\n",
    "\n",
    "            # Apply the Residual connection (the first Conv2D-BN-ReLU block does not use one)\n",
    "            if i > 0:\n",
    "                current_tensor = self._residual_layers[i]([current_tensor, previous_tensor])\n",
    "\n",
    "            # Save the output to use it as an input for the next iteration\n",
    "            previous_tensor = current_tensor\n",
    "\n",
    "        # Uncomment the following line to enable the optional Dropout layer\n",
    "        # return self._dropout_layer(current_tensor, training=training)\n",
    "        \n",
    "        # Return the final output tensor\n",
    "        return current_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalUNET(tf.keras.models.Model):\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_of_prediction_steps: int,\n",
    "        num_of_levels: int, \n",
    "        num_of_filters: list, \n",
    "        conv_blocks_per_level: int = 3, \n",
    "        kernel_size: tuple = (3,3), \n",
    "        leaky_relu_slope: float = 0.10):\n",
    "\n",
    "        \"\"\"\n",
    "        num_of_levels: \n",
    "        num_of_filters:\n",
    "        conv_blocks_per_level:\n",
    "        kernel_size: \n",
    "        leaky_relu_slope: \n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self._num_of_levels = num_of_levels\n",
    "        self._num_of_prediction_steps = num_of_prediction_steps\n",
    "        \n",
    "        # Time-Distributed,  2D-Convolution layers on the encoder path \n",
    "        self._encoder_conv_blocks_2D = [None] * self._num_of_levels\n",
    "\n",
    "        # Time-Distributed, 2D-Convolution layers on the decoder path\n",
    "        self._decoder_conv_blocks_2D = [None] * self._num_of_levels\n",
    "\n",
    "        # 2D-Conv-LSTM layers between the encoder and decoder path \n",
    "        self._lstm_layers_2D = [None] * self._num_of_levels\n",
    "\n",
    "        # Optional BatchNorm layers (they are meant to be used before the ConvLSTM2D layer)\n",
    "        self._batch_norm_layers = [None] * self._num_of_levels\n",
    "        \n",
    "        # Residual connections (channel-wise addition) between Conv-LSTM input and output tensors\n",
    "        self._residual_layers = [None] * self._num_of_levels\n",
    "\n",
    "        # Skip connections (depth-wise concatenation) on the decoder path\n",
    "        self._concatenate_layers = [None] * self._num_of_levels\n",
    "        \n",
    "        # Downsampling layers on the encoder path \n",
    "        self._pooling_layers = [None] * self._num_of_levels\n",
    "\n",
    "        # Upsampling layers on the decoder path \n",
    "        self._upsampling_layers = [None] * self._num_of_levels\n",
    "\n",
    "        # Build the encoder network / contracting path \n",
    "        for i in range(self._num_of_levels): \n",
    "            self._encoder_conv_blocks_2D[i] = ConvBlock2D(\n",
    "                num_of_filters = num_of_filters[i],\n",
    "                num_of_blocks = conv_blocks_per_level, \n",
    "                kernel_size = kernel_size, \n",
    "                leaky_relu_slope = leaky_relu_slope                \n",
    "            )\n",
    "\n",
    "            self._encoder_conv_blocks_2D[i] = tf.keras.layers.TimeDistributed(self._encoder_conv_blocks_2D[i])\n",
    "\n",
    "            # The last resolution level on the encoder path does not require a Pooling layer \n",
    "            if i != self._num_of_levels - 1: \n",
    "                self._pooling_layers[i] = tf.keras.layers.MaxPooling2D(pool_size = (2,2), padding = \"valid\")\n",
    "                self._pooling_layers[i] = tf.keras.layers.TimeDistributed(self._pooling_layers[i])\n",
    "\n",
    "        # Add Conv2D-LSTM layers and residual connections between the corresponding encoder and decoder levels.\n",
    "        for i in range(self._num_of_levels):\n",
    "            self._lstm_layers_2D[i] = tf.keras.layers.ConvLSTM2D(\n",
    "                filters = num_of_filters[i],\n",
    "                kernel_size = kernel_size, \n",
    "                padding = \"same\",\n",
    "                return_sequences = True, \n",
    "                return_state = False, \n",
    "                dropout = 0.0,\n",
    "                recurrent_dropout = 0.0,\n",
    "            )\n",
    "\n",
    "            self._batch_norm_layers[i] = tf.keras.layers.BatchNormalization()\n",
    "            self._residual_layers[i] = tf.keras.layers.Add()\n",
    "\n",
    "        # Build the decoder network / expanding path\n",
    "        for i in range(self._num_of_levels): \n",
    "            self._decoder_conv_blocks_2D[i] = ConvBlock2D(\n",
    "                num_of_filters = num_of_filters[i], \n",
    "                num_of_blocks = conv_blocks_per_level, \n",
    "                kernel_size = kernel_size, \n",
    "                leaky_relu_slope = leaky_relu_slope\n",
    "            )\n",
    "\n",
    "            self._decoder_conv_blocks_2D[i] = tf.keras.layers.TimeDistributed(self._decoder_conv_blocks_2D[i])\n",
    "\n",
    "            # The first resolution level on the decoder path does not require an Upsampling layer\n",
    "            if i != 0: \n",
    "                self._upsampling_layers[i] = tf.keras.layers.UpSampling2D(size = (2,2), interpolation = \"bilinear\")\n",
    "                self._upsampling_layers[i] = tf.keras.layers.TimeDistributed(self._upsampling_layers[i])\n",
    "\n",
    "            # The last resolution level does not require a channel-wise, concatenation layer\n",
    "            # TODO: Add concatenation axis (channel dimension)\n",
    "            if i != self._num_of_levels - 1: \n",
    "                self._concatenate_layers[i] = tf.keras.layers.Concatenate(axis = -1)\n",
    "\n",
    "        self._output_layer = tf.keras.layers.ConvLSTM2D(\n",
    "            filters = 2, \n",
    "            kernel_size = kernel_size, \n",
    "            padding = \"same\", \n",
    "            activation = \"sigmoid\", \n",
    "            return_sequences = False,\n",
    "            return_state = False,\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, training = False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        temp = inputs\n",
    "        \n",
    "        encoder_outputs = [None] * self._num_of_levels\n",
    "        lstm_outputs = [None] * self._num_of_levels\n",
    "        decoder_outputs = [None] * self._num_of_levels\n",
    "\n",
    "        # Define the computation graph of the contracting path / encoder network\n",
    "        for i in range(self._num_of_levels): \n",
    "            print(\"encoder path: {}\".format(i))\n",
    "            encoder_outputs[i] = self._encoder_conv_blocks_2D[i](temp, training = training)\n",
    "            print(tf.keras.backend.int_shape(encoder_outputs[i]))\n",
    "            \n",
    "            if i != self._num_of_levels - 1:\n",
    "                temp = self._pooling_layers[i](encoder_outputs[i])\n",
    "\n",
    "        print(\"\")\n",
    "        \n",
    "        # Define the computation graph of the ConvLSTM layers between the encoder and decoder\n",
    "        for i in range(self._num_of_levels): \n",
    "            print(\"LSTM path: {}\".format(i))\n",
    "\n",
    "            # Uncomment to enable BatchNormalization layers before ConvLSTM layers\n",
    "            # lstm_outputs[i] = self._batch_norm_layers[i](encoder_outputs[i], training = trainin)\n",
    "            # lstm_outputs[i] = self._lstm_layers_2D[i](lstm_outputs[i], training = training)\n",
    "            # lstm_outputs[i] = self._residual_layers[i]([encoder_outputs[i], lstm_outputs[i]])\n",
    "            \n",
    "            lstm_outputs[i] = self._lstm_layers_2D[i](encoder_outputs[i], training = training)\n",
    "            lstm_outputs[i] = self._residual_layers[i]([encoder_outputs[i], lstm_outputs[i]])\n",
    "            print(tf.keras.backend.int_shape(lstm_outputs[i]))\n",
    "\n",
    "        print(\"\")\n",
    "        \n",
    "        # Define the computation graph of the expanding path / decoder network\n",
    "        for i in range(self._num_of_levels - 1, -1, -1):\n",
    "            print(\"Decoder path: {}\".format(i))\n",
    "            temp = lstm_outputs[i]\n",
    "            \n",
    "            # Concatenation\n",
    "            if i != self._num_of_levels - 1:\n",
    "                temp = self._concatenate_layers[i]([lstm_outputs[i], decoder_outputs[i+1]])\n",
    "\n",
    "            print(\"concat shape: \" + str(tf.keras.backend.int_shape(temp)))\n",
    "            \n",
    "            # Convolution \n",
    "            decoder_outputs[i] = self._decoder_conv_blocks_2D[i](temp, training = training)\n",
    "\n",
    "            print(\"conv shape: \" + str(tf.keras.backend.int_shape(decoder_outputs[i])))\n",
    "\n",
    "            # Upsampling \n",
    "            if i != 0:\n",
    "                decoder_outputs[i] = self._upsampling_layers[i](decoder_outputs[i])\n",
    "\n",
    "            print(\"upsampling size: \" + str(tf.keras.backend.int_shape(decoder_outputs[i])))\n",
    "\n",
    "        output_tensor = self._output_layer(decoder_outputs[0], training = training)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"tensor output: \" + str(tf.keras.backend.int_shape(output_tensor)))\n",
    "        \n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder path: 0\n",
      "(None, 10, 512, 512, 8)\n",
      "encoder path: 1\n",
      "(None, 10, 256, 256, 12)\n",
      "encoder path: 2\n",
      "(None, 10, 128, 128, 16)\n",
      "encoder path: 3\n",
      "(None, 10, 64, 64, 24)\n",
      "encoder path: 4\n",
      "(None, 10, 32, 32, 32)\n",
      "\n",
      "LSTM path: 0\n",
      "(None, 10, 512, 512, 8)\n",
      "LSTM path: 1\n",
      "(None, 10, 256, 256, 12)\n",
      "LSTM path: 2\n",
      "(None, 10, 128, 128, 16)\n",
      "LSTM path: 3\n",
      "(None, 10, 64, 64, 24)\n",
      "LSTM path: 4\n",
      "(None, 10, 32, 32, 32)\n",
      "\n",
      "Decoder path: 4\n",
      "concat shape: (None, 10, 32, 32, 32)\n",
      "conv shape: (None, 10, 32, 32, 32)\n",
      "upsampling size: (None, 10, 64, 64, 32)\n",
      "Decoder path: 3\n",
      "concat shape: (None, 10, 64, 64, 56)\n",
      "conv shape: (None, 10, 64, 64, 24)\n",
      "upsampling size: (None, 10, 128, 128, 24)\n",
      "Decoder path: 2\n",
      "concat shape: (None, 10, 128, 128, 40)\n",
      "conv shape: (None, 10, 128, 128, 16)\n",
      "upsampling size: (None, 10, 256, 256, 16)\n",
      "Decoder path: 1\n",
      "concat shape: (None, 10, 256, 256, 28)\n",
      "conv shape: (None, 10, 256, 256, 12)\n",
      "upsampling size: (None, 10, 512, 512, 12)\n",
      "Decoder path: 0\n",
      "concat shape: (None, 10, 512, 512, 20)\n",
      "conv shape: (None, 10, 512, 512, 8)\n",
      "upsampling size: (None, 10, 512, 512, 8)\n",
      "\n",
      "tensor output: (None, 512, 512, 2)\n",
      "Model: \"temporal_unet_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_90 (TimeD  multiple                  410       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_92 (TimeD  multiple                  788       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_94 (TimeD  multiple                  1244      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_96 (TimeD  multiple                  2344      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_98 (TimeD  multiple                  3896      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_99 (TimeD  multiple                  572       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_100 (Time  multiple                  1048      \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " time_distributed_102 (Time  multiple                  1720      \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " time_distributed_104 (Time  multiple                  3344      \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " time_distributed_106 (Time  multiple                  4160      \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " conv_lstm2d_30 (ConvLSTM2D  multiple                  4640      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_31 (ConvLSTM2D  multiple                  10416     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_32 (ConvLSTM2D  multiple                  18496     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_33 (ConvLSTM2D  multiple                  41568     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_34 (ConvLSTM2D  multiple                  73856     \n",
      " )                                                               \n",
      "                                                                 \n",
      " add_135 (Add)               multiple                  0         \n",
      "                                                                 \n",
      " add_136 (Add)               multiple                  0         \n",
      "                                                                 \n",
      " add_137 (Add)               multiple                  0         \n",
      "                                                                 \n",
      " add_138 (Add)               multiple                  0         \n",
      "                                                                 \n",
      " add_139 (Add)               multiple                  0         \n",
      "                                                                 \n",
      " concatenate_20 (Concatenat  multiple                  0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " concatenate_21 (Concatenat  multiple                  0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " concatenate_22 (Concatenat  multiple                  0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " concatenate_23 (Concatenat  multiple                  0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " time_distributed_91 (TimeD  multiple                  0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_93 (TimeD  multiple                  0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_95 (TimeD  multiple                  0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_97 (TimeD  multiple                  0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_101 (Time  multiple                  0         \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " time_distributed_103 (Time  multiple                  0         \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " time_distributed_105 (Time  multiple                  0         \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " time_distributed_107 (Time  multiple                  0         \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " conv_lstm2d_35 (ConvLSTM2D  multiple                  728       \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 169230 (661.05 KB)\n",
      "Trainable params: 168126 (656.74 KB)\n",
      "Non-trainable params: 1104 (4.31 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TemporalUNET(num_of_prediction_steps=1, num_of_levels = 5, num_of_filters = [8, 12, 16, 24, 32]) \n",
    "model.build(input_shape=(None, 10, 512, 512, 2))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAgU0YIiDo7g",
    "outputId": "a4f6cd3f-5ce1-4f6a-c75c-ee3e5845b4c5"
   },
   "outputs": [],
   "source": [
    "class EncoderDEM(tf.keras.models.Model):\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements a Convolutional Encoder architecture which is meant to be \n",
    "    used as a feature extractor for Digital Elevation Maps. Feature maps are extracted \n",
    "    sequentially starting from high spatial resolution and proceeding to lower\n",
    "    spatial resolutions with Downsampling/Pooling layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_of_levels: int,                 # Total number of resolution levels. Every level feeds its output to the next level through a pooling layer\n",
    "        num_of_filters: list,               # Number of kernels per convolution layer. See class ConvBlock2D for more\n",
    "        conv_blocks_per_level: int = 3,     # Number of convolution layers per resolution level. See class ConvBlock2D for more\n",
    "        kernel_size: tuple = (3,3),         # Kernel dimensions (height x width). See class ConvBlock2D for more\n",
    "        leaky_relu_slope: float = 0.10):    # (float) ReLU slope for negative input/arguments. See class ConvBlock2D for more\n",
    "        \n",
    "        \"\"\"\n",
    "        Description: \n",
    "        Class Constructor\n",
    "\n",
    "        Arguments: \n",
    "         - levels: (int)\n",
    "         - conv_blocks_per_level: (int)\n",
    "         - kernel_size: (tuple)\n",
    "         - leaky_relu_slope: (float)\n",
    "        \"\"\"\n",
    "\n",
    "        # Store total number of resolution levels as a seperate class attribute \n",
    "        self._num_of_levels = num_of_levels\n",
    "\n",
    "        # An empty list to hold all ConvBlock2D instances\n",
    "        self._conv_blocks_2D = [None] * self._num_of_levels\n",
    "\n",
    "        # An empty list to hold all MaxPooling2D layer instances\n",
    "        self._pooling_layers = [None] * self._num_of_levels\n",
    "\n",
    "        # Fill in the empty lists with keras.Layer instances\n",
    "        for i in range(self._num_of_levels): \n",
    "            self._conv_blocks_2D[i] = ConvBlock2D(\n",
    "                num_of_filters = num_of_filters[i],\n",
    "                num_of_blocks = conv_blocks_per_layer, \n",
    "                kernel_size = kernel_size, \n",
    "                leaky_relu_slope = leaky_relu_slope\n",
    "            )\n",
    "\n",
    "            # The last resolution level does not perform a pooling operation on its output\n",
    "            if i != self._num_of_blocks - 1: \n",
    "                self._pooling_layer[i] = tf.keras.layer.MaxPooling2D(pool_size=(2,2), padding=\"valid\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        # An empty list to hold the feature maps \n",
    "        outputs = [None] * self._num_of_levels\n",
    "\n",
    "        # Placeholder variable for storing the output of the intermediate max-pooling layers\n",
    "        temp = inputs\n",
    "        \n",
    "        for i in range(self._num_of_levels): \n",
    "            outputs[i] = self._conv_blocks_2D[i](temp, training)\n",
    "\n",
    "            # The last conv-block does not feed its output to a pooling layer\n",
    "            if i != self._num_of_levels - 1:\n",
    "                temp = self._pooling_layers[i](outputs[i])\n",
    "                                                 \n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
